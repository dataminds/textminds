# 분석3: 주제모형 {#anal3topic}


## 개관

문서에 빈번하는 등장하는 단어를 통해 그 문서의 주제를 추론할 수 있다. 한 문서에는 다양한 주제가 들어있다. 

예를 들어, 아래 문장은 AP가 보도한 1988년 허스트재단의 링컨센터 기부 기사다.

```{r topic1}
ap_v <- c("The William Randolph Hearst Foundation will give $1.25 million to Lincoln Center, Metropolitan Opera Co., New York Philharmonic and Juilliard School. “Our board felt that we had a
real opportunity to make a mark on the future of the performing arts with these grants an act
every bit as important as our traditional areas of support in health, medical research, education
and the social services,” Hearst Foundation President Randolph A. Hearst said Monday in
announcing the grants. Lincoln Center’s share will be $200,000 for its new building, which
will house young artists and provide new public facilities. The Metropolitan Opera Co. and
New York Philharmonic will receive $400,000 each. The Juilliard School, where music and
the performing arts are taught, will get $250,000. The Hearst Foundation, a leading supporter
of the Lincoln Center Consolidated Corporate Fund, will make its usual annual $100,000
donation, too.")

```


단어의 총빈도와 상대빈도를 계산하면, 이 문서의 주요 내용이 무엇인지 파악할 수 있다. 
먼저 총빈도 상위단어를 찾아보자.

```{r topic2, message=FALSE, warning=FALSE}
pkg_v <- c("tidyverse", "tidytext", "tidylo")
lapply(pkg_v, require, ch = T)

```

```{r topic3}
ap_count <- 
ap_v %>% tibble(text = .) %>% 
  unnest_tokens(word, text, drop = F) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = T) %>% head(10)

```

상대빈도를 구해보자.

먼저 문장 단위로 토큰화해 문장별 ID를 구한다음 단어 단위로 토큰화한다. 대문자 앞의 마침표와 공백(`"\\.\\s(?=[:upper:])"`)을 기준으로 구분하면 된다.  

```{r topic4}
ap_tfidf <- 
ap_v %>% tibble(text = .) %>% 
  mutate(text = str_squish(text)) %>% 
  unnest_tokens(sentence, text, token = "regex", pattern = "\\.\\s(?=[:upper:])") %>% 
  mutate(ID = row_number()) %>% 
  unnest_tokens(word, sentence, drop = F) %>% 
  anti_join(stop_words) %>% 
  count(ID, word, sort = T) %>% 
  bind_tf_idf(term = word, document = ID, n = n) %>% 
  arrange(-tf_idf) %>% head(10)

ap_wlo <- 
ap_v %>% tibble(text = .) %>% 
  mutate(text = str_squish(text)) %>% 
  unnest_tokens(sentence, text, token = "regex", pattern = "\\.\\s(?=[:upper:])") %>% 
  mutate(ID = row_number()) %>% 
  unnest_tokens(word, sentence, drop = F) %>% 
  anti_join(stop_words) %>% 
  count(ID, word, sort = T) %>% 
  bind_log_odds(feature = word, set = ID, n = n) %>% 
  arrange(-log_odds_weighted) %>% head(10)

```

추출한 상위 10대 빈도 단어를 비교해 보자. 

```{r topic5}
bind_cols(
  select(ap_count,  총빈도  = word),
  select(ap_tfidf, tf_idf = word),
  select(ap_wlo,  가중승산비  = word)
)
```

총빈도와 상대빈도를 보면, 허스트재단이 링컨아트센터에 기부금 발표한 내용이란 것을 추론할 수 있지만, 기사에는 보다 다양한 주제를 담고 있다. 기사의 내용을 읽어보면 다양한 주제가 있음을 알수 있다 (Figure \@ref(fig:ap)).

```{r echo=F, ap, fig.cap="AP 기사", dpi=30}
knitr::include_graphics("images/ap.png")

```
기사에 포함된 단어 중 같은 색으로 구부된 단어들을 모아보면 예술, 재정, 아동, 교육 등의 주제를 나타내는 일관된 단어로 구성됐음을 알수 있다 (Figure \@ref(fig:ap2)). 

```{r echo=F, ap2, fig.cap="AP 기사 주제", dpi=30}
knitr::include_graphics("images/ap2.png")

```

Blei 등 일군의 전산학자들은 문서 내 단어의 확률분포를 계산해 찾아낸 일련의 단어 군집을 통해 문서의 주제를 추론하는 방법으로서 LDA(Latent Dirichlet Allocation)을 제시했다. 

- Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Research, 3, 993-1022.

19세기 독일 수학자 러죈 디리클레(Lejeune Dirichlet, 1805 ~ 1859)가 제시한 디리클래 분포(Dirichlet distribution)를 이용해 문서에 잠재된 주제를 추론하기에 잠재 디리클레 할당(LDA: Latent Dirichlet Allocation)이라고 했다. 문서의 주제를 추론하는 방법이므로 주제모형(topic models)이라고 한다. 

Beli(2012)가 설명한 LDA에 대한 직관적인 이해는 다음과 같다. Figure\@ref(fig:topic)에 제시된 논문 "Seeking life's bare (genetics) necessities"은 진화의 틀에서 유기체가 생존하기 위해 필요한 유전자의 수를 결정하기 위한 데이터분석에 대한 내용이다. 문서(documents)에 파란색으로 표시된 'computer' 'prediction' 등은 데이터분석에 대한 단어들이다. 분홍색으로 표시된 'life' 'organism'은 진화생물학에 대한 내용이다. 노란색으로 표시된 'sequenced' 'genes'는 유전학에 대한 내용이다. 이 논문의 모든 단어를 이런 식으로 분류하면 아마도 이 논문은 유전학, 데이터분석, 진화생물학 등이 상이한 비율로 혼합돼 있음을 알게 된다. 

- Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84.

```{r echo=F, dpi=30, topic, fig.cap="LDA의 직관적 예시"}
knitr::include_graphics("images/topic.jpg")

```

LDA에는 다음과 같은 전제가 있다. 

- 말뭉치에는 단어를 통해 분포된 다수의 주제가 있다 (위 그림의 가장 왼쪽). 

각 문서에서 주제를 생성하는 과정은 다음과 같다. 

- 주제에 대해 분포 선택(오른쪽 히스토그램)
- 각 단어에 대해 주제의 분포 선택(색이 부여된 동그라미)
- 해당 주제를 구성하는 단어 선택(가운데 화살표)

LDA에서 정의하는 주제(topic)는 특정 단어에 대한 분포다. 예를 들어, 유전학 주제라면 유전학에 대하여 높은 확률로 분포하는 단어들이고, 진화생물학 주제라면 진화생물학에 대하여 높은 확률로 분포하는 단어들이다. 

LDA에서는 문서를 주머니에 무작위로 섞여 있는 임의의 혼합물로 본다(Bag of words). 일반적으로 사용하는 문장처럼 문법이라는 짜임새있는 구조로 보는 것이 아니다. 임의의 혼합물이지만 온전하게 무작위로 섞여 있는 것은 아니다. 서로 함께 모여 있는 군집이 확률적으로 존재한다. 즉, 주제모형에서 접근하는 문서는 잠재된 주제의 혼합물로서, 각 주제를 구성하는 단어 단위가 확률적으로 혼합된 주머니(bag)인 셈이다.

- 개별 문서: 여러 주제(topic)가 섞여 있는 혼합물
  - 문서마다 주제(예술, 교육, 예산 등)의 분포 비율 상이
  - 주제(예: 예술)마다 단어(예: 오페라, 교향악단)의 분포 상이

주제모형의 목표는 말뭉치에서 주제의 자동추출이다. 문서 자체는 관측가능하지만, 주제의 구조(문서별 주제의 분포와 문서별-단어별 주제할당)는 감춰져 있다. 감춰진 주제의 구조를 찾아내는 작업은 뒤집어진 생성과정이라고 할 수 있다. 관측된 말뭉치를 생성하는 감춰진 구조를 찾아내는 작업이기 때문이다. 문서에 대한 사전 정보없이 문서의 주제를 분류하기 때문에 주제모형은 비지도학습(unsupervised learning) 방식의 기계학습(machine learning)이 된다. 

- 기계학습(machine learing)
  - 인공지능 작동방식. 투입한 데이터에서 규칙성 탐지해 분류 및 예측. 지도학습, 비지도학습, 강화학습 등으로 구분.  
- 지도학습(supervised learning)
  - 인간이 사전에 분류한 결과를 학습해 투입한 자료에서 규칙성 혹은 경향  발견
- 비지도학습(unsupervised learning)  
  - 사전분류한 결과 없이 기계 스스로 투입한 자료에서 규칙성 혹은 경향 발견
- 강화학습(reinforced learning)
  - 행동의 결과에 대한 피드백(보상, 처벌 등)을 통해 투입한 자료에서 규칙성 혹은 경향 발견

주제모형의 효용은 대량의 문서에서 의미구조를 닮은 주제구조를 추론해 주석을 자동으로 부여할 수 있다는데 있다. 

주제모형은 다양한 패지키가 있다. 

- [lda](https://cran.r-project.org/web/packages/lda/)
- [topicmodels](https://cran.r-project.org/web/packages/topicmodels/) 
- [stm](https://cran.r-project.org/web/packages/stm/)

여기서는 구조적 주제모형(structural topic model)이 가능한 `stm`패키지를 이용한다. `stm`은 메타데이터를 이용한 추출한 주제에 대하여 다양한 분석을 할 수 있는 장점이 있다. 



## 자료 준비

[빅카인즈](https://www.bigkinds.or.kr/)에서 다음의 조건으로 기사를 추출한다.

- 검색어: 인공지능
- 기간: 2010.1.1 ~ 2020.12.31
- 언론사: 중앙일보, 조선일보, 한겨레, 경향신문, 한국경제, 매일경제
- 분석: 분석 기사

모두 5145건이다. 

### 자료 이입 

다운로드한 기사를 작업디렉토리 아래 `data`폴더에 복사한다. 

```{r topic9}
list.files("data/.")

```

파일명이 'NewsResult_20200101-20201201.xlsx'다. 

```{r topic10}
readxl::read_excel("data/NewsResult_20200101-20201231.xlsx") %>% names()

```

분석에 필요한 열을 선택해 데이터프레임으로 저장한다. 분석 텍스트는 제목과 본문이다. 빅카인즈는 본문을 200자까지만 무료로 제공하지만, 학습 목적을 달성하기에는 충분하다. 제목은 본문의 핵심 내용을 반영하므로, 제목과 본문을 모두 주제모형 분석에 투입한다. 시간별, 언론사별, 분류별로 분석할 계획이므로, 해당 열을 모두 선택한다. 

```{r topic11}
ai_df <- 
readxl::read_excel("data/NewsResult_20200101-20201231.xlsx") %>% 
  select(일자, 제목, 본문, 언론사, cat = `통합 분류1`) 
ai_df %>% head()

```

시간열에는 연월일의 값이 있다. 월별 추이에 따른 주제를 분석할 것이므로 열의 값을 월에 대한 값으로 바꾼다. `tidyverse`패키지에 함께 설치되는 `lubridate`패키지를 이용해 문자열을 날짜형으로 변경하고 월 데이터 추출해 새로운 열 `month` 생성한다. `lubridate`는 `tidyverse`에 포함돼 있으나 함께 부착되지 않으므로 별도로 실행해야 한다. 

```{r topic12}
library(lubridate)
as_date("20201231") %>% month()
ymd("20201231") %>% month()

```

DB에 같은 기사가 중복 등록되는 경우가 있으므로, `dplyr`패키지의 `distinct()`함수를 이용해 중복된 문서를 제거한다. `.keep_all = `인자의 기본값은 `FALSE`다. 투입한 열 이외의 열은 유지하지 않는다. 다른 변수(열)도 분석에 필요하므로  `.keep_all = `인자를 `TRUE`로 지정한다. 

분석 목적에 맞게 열을 재구성한다. 

```{r topic13}
library(lubridate) 

ai2_df <- 
ai_df %>% 
  # 중복기사 제거
  distinct(제목, .keep_all = T) %>% 
  # 기사별 ID부여
  mutate(ID = factor(row_number())) %>% 
  # 월별로 구분한 열 추가
  mutate(month = month(ymd(일자))) %>% 
  # 기사 제목과 본문 결합
  unite(제목, 본문, col = "text", sep = " ") %>% 
  # 중복 공백 제거
  mutate(text = str_squish(text)) %>% 
  # 언론사 분류: 보수 진보 경제 %>% 
  mutate(press = case_when(
    언론사 == "조선일보" ~ "종합지",
    언론사 == "중앙일보" ~ "종합지",
    언론사 == "경향신문" ~ "종합지",
    언론사 == "한겨레" ~ "종합지",
    언론사 == "한국경제" ~ "경제지",
    TRUE ~ "경제지") ) %>% 
  # 기사 분류 구분 
  separate(cat, sep = ">", into = c("cat", "cat2")) %>% 
  # IT_과학, 경제, 사회 만 선택
  filter(str_detect(cat, "IT_과학|경제|사회")) %>% 
  select(-cat2)  

ai2_df %>% head(5)
ai2_df %>% names()

```

기사의 분류된 종류, 월 등 새로 생성한 열의 내용을 확인해보자.

```{r topic14}
ai2_df$cat %>% unique()
ai2_df$month %>% unique()
ai2_df$press %>% unique()

```

분류별, 월별로 기사의 양을 계산해보자.

```{r topic15}
ai2_df %>% count(cat, sort = T)
ai2_df %>% count(month, sort = T)
ai2_df %>% count(press, sort = T)

```


### 정제

#### 토큰화

`KoNLP`패키지의 `extractNoun()`함수로 명사만 추출해 토큰화한다. 명사가 문서의 주제를 잘 나타내므로 주제모형에서는 주로 명사를 이용하지만, 목적에 따라서는 다른 품사(용언 등)를 분석에 투여하기도 한다. 

(* 형태소 추출전에 문자 혹은 공백 이외의 요소(예: 구둣점)를 먼저 제거한다.) 

```{r topic16}
library(KoNLP)
ai_tk <- 
ai2_df %>% 
  mutate(text = str_remove_all(text, "[^(\\w+|\\s)]")) %>%  # 문자 혹은 공백 이외 것 제거
  unnest_tokens(word, text, token = extractNoun, drop = F) 
ai_tk %>% glimpse()

```


#### 불용어 제거
'인공지능'으로 검색한 기사이므로, '인공지능'관련 단어는 제거한다. 
문자가 아닌 요소를 모두 제거한다. (숫자를 반드시 제거해야 하는 것은 아니다.)

```{r topic17}
ai_tk <- 
ai_tk %>% 
  filter(!word %in% c("인공지능", "AI", "ai", "인공지능AI", "인공지능ai")) %>% 
  filter(str_detect(word, "[:alpha:]+")) 

```

단어의 총빈도와 상대빈도를 살펴보자

```{r topic18}
ai_tk %>% count(word, sort = T)

```

상대빈도가 높은 단어와 낮은 단어를 확인한다. 

```{r topic19}
ai_tk %>% count(cat, word, sort = T) %>% 
  bind_log_odds(set = cat, feature = word, n = n) %>% 
  arrange(log_odds_weighted)

ai_tk %>% count(cat, word, sort = T) %>% 
  bind_tf_idf(term = word, document = word, n = n) %>% 
  arrange(idf)

```

한글자 단어는 문서의 주제를 나타내는데 기여하지 못하는 경우도 있고, 고유명사인데 형태소로 분리돼 있는 경우도 있다. 상대빈도가 높은 단어를 살펴 특이한 단어가 있으면 형태소 추출전 단어가 무엇인지 확인한다. 특이한 경우가 없으면 한글자 단어는 모두 제거한다.
tibble데이터프레임은 문자열의 일부만 보여준다.  `pull()`함수로 열에 포함된 문자열을 벡터로 출력하므로, 모든 내용을 확인할 수 있다. 

```{r topic20}
ai_tk %>% 
  filter(word == "하") %>% pull(text) %>% head(3)

```


'기업' '기술' 등의 단어는 총사용빈도가 높지만, 상대빈도는 낮다. 대부분의 분류에서 널리 사용된 단어다. 지금 제거할필요는 없지만, 제거가능성을 염두에 둔다. 

```{r topic21}
ai_tk %>% 
  filter(str_length(word) > 1) -> ai2_tk

ai2_tk %>% 
  count(word, sort = T) 

```

상대빈도를 다시 확인하자. 

```{r topic22}
ai2_tk %>% count(cat, word, sort = T) %>% 
  bind_log_odds(set = cat, feature = word, n = n) %>% 
  arrange(-log_odds_weighted)

ai2_tk %>% count(cat, word, sort = T) %>% 
  bind_tf_idf(term = word, document = word, n = n) %>% 
  arrange(tf_idf)

```



### stm 말뭉치


```{r topic23, eval=FALSE}
install.packages("stm", dependencies = T)

```

토큰화한 데이터프레임을 `stm`패키지 형식의 말뭉치로 변환한다. 이를 위해 먼저 분리된 토큰을 원래 문장에 속한 하나의 열로 저장한다.

- `str_flatten()`은 `str_c()`함수와 달리, 문자열을 결합해 단일 요소로 산출한다.  
  - `str_c()`함수에  `collapse = `인자를 사용한 경우에 해당한다.  
  - `str_c()`는 R기본함수 `paste0()`와 비슷하다. 결측값 처리방법이 서로 다르다. 


```{r topic24}
combined_df <-
  ai2_tk %>%
  group_by(ID) %>%
  summarise(text2 = str_flatten(word, " ")) %>%
  ungroup() %>% 
  inner_join(ai2_df, by = "ID")

combined_df %>% glimpse()

```

`textProcessor()`함수로 리스트 형식의 stm말뭉치로 변환한다. 'documents' 'vacab' 'meta'등의 하부요소가 생성된다. 'meta'에 텍스트데이터가 저장돼 있다. 

영문문서의 경우, `textProcessor()`함수로 정제과정을 수행하므로, 한글문서처럼 별도의 형태소 추출과정 바로 영문데이터프레임을 투입하면 된다. 

'text2'열에 토큰화한 단어가 저장돼 있다. 

만일 `tm`패키지나 `SnowballC`패키지가 설치돼 있지 않으면 

```{r topic25}
library(stm)
processed <- 
  ai2_df %>% textProcessor(documents = combined_df$text2, metadata = .)

```

`prepDocuments()`함수로 주제모형에 사용할 데이터의 인덱스(wordcounts)를 만든다.

```{r topic26}
out <- 
  prepDocuments(processed$documents,
                     processed$vocab,
                     processed$meta)

```


제거할수 있는 단어와 문서의 수를 `plotRemoved()`함수로 확인할 수 있다. 

```{r topic27}
plotRemoved(processed$documents, lower.thresh = seq(0, 100, by = 5))

```

`lower.thresh = `로 최소값 설정하면 빈도가 낮아 제거할 용어의 수를 설정할 수 있다. 설정값을 너무 높게 잡으면 분석의 정확도가 떨어진다. 여기서는 계산 편의를 위해 설정값을 높게 잡았다. 

```{r topic28}
out <-
  prepDocuments(processed$documents,
                processed$vocab,
                processed$meta, 
                lower.thresh = 15)

```

산출결과를 개별 객체로 저장한다. 이 객체들은 이후 모형구축에 사용된다. 

```{r topic29}
docs <- out$documents
vocab <- out$vocab
meta <- out$meta

```


## 분석

### 주제의 수(K) 설정

주제를 몇개로 설정할지 탐색한다. 7개와 10개를 놓고 비교해보자. `searchK()`함수는 주제의 수에 따라 4가지 계수를 제공한다. 

- 배타성(exclusivity): 특정 주제에 등장한 단어가 다른 주제에는 나오지 않는 정도. 확산타당도에 해당. 
- 의미 일관성(semantic coherence): 특정 주제에 높은 확률로 나타나는 단어가 동시에 등장하는 정도. 수렴타당도에 해당.

- 지속성(heldout likelihood): 데이터 일부가 존재하지 않을 때의 모형 예측 지속 정도. 

- 잔차(residual): 투입한 데이터로 모형을 설명하지 못하는 오차. 

배타성, 의미 일관성, 지속성이 높을 수록, 그리고 잔차가 작을수록 모형의 적절성 증가. 

보통 10개부터 100개까지 10개 단위로 주제의 수를 구분해 연구자들이 정성적으로 최종 주제의 수 판단한다. 

학습 상황이므로 계산시간을 줄이기 위해 주제의 수를 3개와 10개의 결과만 비교한다. (iteration을 200회 이상 수행하므로 계산시간이 오래 걸린다.)

백그라운드에서 계산하도록 하면, RStudio에서 다른 작업을 병행할 수 있다. 

- [RStudio 코드 백그라운드에서 돌리기](https://www.youtube.com/watch?v=84rNE63H8gA)
 

```{r  topic30, message=FALSE, warning=FALSE}
# topicN <- seq(from = 10, to = 100, by = 10)
topicN <- c(3, 10)

storage <- searchK(out$documents, out$vocab, K = topicN)
plot(storage)

```

배타성, 지속성, 잔차 등 3개 지표에서 모두 주제의 수가 10개인 모형이 3개인 모형보다 우수하고, 3개인 모형은 의미일관성만 높다. 따라서 이미 10개로 분석한 모형을 그대로 이용한다. 


### 주제모형 구성

docs, vocab, meta에 저장된 문서와 텍스트정보를 이용해 주제모형을 구성한다. 추출한 주제의 수는 `K = `인자로 설정한다. 처음에는 임의의 값을 투입한다. 이후 적절한 주제의 수를 다시 추정하는 단계가 있다. 
모형 초기값은 `init.type = `인자에 "Spectral"을 투입한다. 같은 결과가 나오도록 하려면  `seed = `인자를 지정한다. 투입하는 값은 개인의 선호대로 한다. 


```{r topic31}
stm_fit <-
  stm(
    documents = docs,
    vocab = vocab,
    K = 10,    # 토픽의 수
    data = meta,
    init.type = "Spectral",
    seed = 37 # 반복실행해도 같은 결과가 나오게 난수 고정
  )

summary(stm_fit) %>% glimpse()
summary(stm_fit)
```

`stm`패키지는 4종의 가중치를 이용해 주제별로 주요 단어를 제시한다. 

- Highest probability: 각 주제별로 단어가 등장할 확률이 높은 정도. 베타(\beta) 값. 
- FREX: 전반적인 단어빈도의 가중 평균. 해당 주제에 배타적으로 포함된 정도.
- Lift: 다른 주제에 덜 등장하는 정도. 해당 주제에 특정된 정도.
- score: 다른 주제에 등장하는 로그 빈도. 해당 주제에 특정된 정도.

FREX와 Lift는 드문 빈도의 단어도 분류하는 경향이 있으므로, 주로 Highest probability(베타)를 이용한다. 

stm패키지의 자세한 내용은 패키지 저자 Roberts의 [stm홈페이지](https://www.structuraltopicmodel.com/)와 해설 논문을 참조한다.

- [stm: R Package for Structural Topic Models](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf)

- [Roberts, M. E., Stewart, B. M., & Airoldi, E. M. (2016). A model of text for experimentation in the social sciences. Journal of the American Statistical Association, 111(515), 988-1003.](https://scholar.princeton.edu/sites/default/files/bstewart/files/stm.pdf)



## 소통

분석한 모형을 통해 말뭉치에 포함된 주제와 주제별 단어의 의미가 무엇인지 전달하기 위해서는 우선 모형에 대한 해석을 제시할 수 있는 시각화가 필요하다. 이를 위해서는 먼저 주요 계수의 의미를 이해할 필요가 있다. 주제모형에서 주제별 확률분포를 나타내는 베타와 감마다. 

- 베타 $\beta$: 단어가 각 주제에 등장할 확률. 각 단어별로 베타 값 부여. stm모형 요약에서 제시한 Highest probability의 지표다. 

- 감마 $\gamma$: 문서가 각 주제에 등장할 확률. 각 문서별로 감마 값 부여. 

즉, 베타와 감마 계수를 이용해 시각화하면 주제와 주제단어의 의미를 간명하게 나타낼 수 있다. 

먼저 `tidy()`함수를 이용해 `stm()`함수로 주제모형을 계산한 결과를 정돈텍스트 형식으로 변환한다([줄리아 실기의 시각화 참고](https://juliasilge.com/blog/evaluating-stm/))


학습편의를 위해 주제의 수롤 6개로 조정해 다시 모형을 구성하자. 

```{r topic32}
stm_fit <-
  stm(
    documents = docs,
    vocab = vocab,
    K = 6,    # 토픽의 수
    data = meta,
    init.type = "Spectral",
    seed = 37, # 반복실행해도 같은 결과가 나오게 난수 고정
    verbose = F
  )

summary(stm_fit) %>% glimpse()
summary(stm_fit)
```

### 주제별 단어 분포

베타 값을 이용해 주제별로 단어의 분포를 막대도표로 시각화하자. 

```{r topic33}
td_beta <- stm_fit %>% tidy(matrix = 'beta') 

td_beta %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 7) %>% 
  ungroup() %>% 
  mutate(topic = str_c("주제", topic)) %>% 
  
  ggplot(aes(x = beta, 
             y = reorder(term, beta),
             fill = topic)) +
  geom_col(show.legend = F) +
  facet_wrap(~topic, scales = "free") +
  labs(x = expression("단어 확률분포: "~beta), y = NULL,
       title = "주제별 단어 확률 분포",
       subtitle = "각 주제별로 다른 단어들로 군집") +
  theme(plot.title = element_text(size = 20))
  
```

### 주제별 문서 분포

감마 값을 이용해 주제별로 문서의 분포를 히스토그램으로 시각화한다. x축과 y축에 각각 변수를 투입하는 막대도표와 달리, 히스토그램은 x축에만 변수를 투입하고, y축에는 x축 값을 구간(bin)별로 요약해 표시한다. 

각 문서가 각 주제별로 등장할 확률인 감마($\gamma$)의 분포가 어떻게 히스토그램으로 표시되는지 살펴보자. 

```{r topic34}
td_gamma <- stm_fit %>% tidy(matrix = "gamma") 
td_gamma %>% glimpse()
td_gamma %>% 
  mutate(max = max(gamma),
         min = min(gamma),
         median = median(gamma))


```

`td_gamma`에는 문서별로 감마 값이 부여돼 있다. 1번 문서는 주제1에 포함될 확률(감마)이 0.4이고, 2번 문서는 주제1에 포함될 확률이 0.2다. 감마 값은 최저 0.04에서 최고 0.74까지 있다. 감마는 연속적인 값이므로 이 감마의 값을 일정한 구간(bin)으로 나누면, 각 감마의 구간에 문서(document)가 몇개 있는지 계산해, 감마 값에 따른 각 문서의 분포를 구할 수 있다. 연속하는 값을 구간(bin)으로 구분해 분포를 표시한 도표가 히스토그램이다. 

주제별로 문서의 분포를 감마 값에 따라 히스토그램으로 시각해하자. `geom_histogram()`함수에서 `bins = `인자의 기본값은 30이다. 즉, bin을 30개로 나눠 분포를 그린다.  

```{r topic35}
td_gamma %>% 
  ggplot(aes(x = gamma, fill = as.factor(topic))) +
  geom_histogram(bins = 100, show.legend = F) +
  facet_wrap(~topic) + 
  labs(title = "주제별 문서 확률 분포",
       y = "문서(기사)의 수", x = expression("문서 확률분포: "~(gamma))) +
  theme(plot.title = element_text(size = 20))
  
```

감마가 높은 문서(기사)가 많지 않고, 대부분 낮은 값에 치우쳐 있다. '인공지능' 단일 검색어로 추출한 말뭉치이기 때문이다. 


### 주제별 단어-문서 분포

주제별로 감마의 평균값을 구하면 비교적 각 주제와 특정 문서와 관련성이 높은 순서로 주제를 구분해 표시할 수 있다. 또한 각 주제 별로 대표 단어를 표시할 수 있다. 가장 간단하게 주제별로 단어와 문서의 분포를 표시하는 방법은 `stm`패키지에서 제공하는 `plot()`함수다. 

`stm` `plot()`함수는  `type = `인자에 'summary' 'labels' 'perspective' 'hist' 등을 투입해 다양한 방식으로 결과를 탐색할 수 있다. 기본적인 정보는 'summary'를 통해 제시한다.  

```{r topic36}
plot(stm_fit, type = "summary", n = 5)
```

위 결과를 `ggplot2` 패키지로 시각화하는 방법은 다음과 같다. 

- 주제별 상위 5개 단어 추출해 데이터프레임에 저장. 
- 문서의 감마 평균값 주제별로 계산해 주제별 상위 단어 데이터프레임과 결합
  
#### 주제별 상위 5개 단어 추출

`td_beta`에서 주제별로 상위 5개 단어 추출해 `top_terms`에 할당한다.  각 주제별로 그룹을 묶어 list형식으로 각 상위단어 5개를 각 주제에 리스트로 묶어준 다음, 다시 데이터프레임의 열로 바꿔준다.  

```{r topic37}
top_terms <- 
td_beta %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 5) %>% 
  select(topic, term) %>% 
  summarise(terms = str_flatten(term, collapse = ", ")) 

```

#### 주제별 감마 평균 계산

`td_gamma`에서 각 주제별 감마 평균값 계산해 `top_terms`(주제별로 추출한 상위 5개 단어 데이터프레임)와 결합해 `gamma_terms`에 할당한다. 

```{r topic38}
gamma_terms <- 
td_gamma %>% 
  group_by(topic) %>% 
  summarise(gamma = mean(gamma)) %>% 
  left_join(top_terms, by = 'topic') %>% 
  mutate(topic = str_c("주제", topic),
         topic = reorder(topic, gamma))

```

결합한 데이터프레임을 막대도표에 표시한다. 문서 확률분포 평균값과 주제별로 기여도가 높은 단어를 표시한다. 주제별로 문서의 확률분포와 단어의 확률분포를 한눈에 볼수 있다.
X축을 0에서 1까지 설정한 이유는 구간을 국소로 설정할 경우, 막대도표가 크기가 상대적으로 크게 보여 결과적으로 데이터의 왜곡이 되기 때문이다. 

```{r topic39}
gamma_terms %>% 
  
  ggplot(aes(x = gamma, y = topic, fill = topic)) +
  geom_col(show.legend = F) +
  geom_text(aes(label = round(gamma, 2)), # 소수점 2자리 
            hjust = 1.4) +                # 라벨을 막대도표 안쪽으로 이동
  geom_text(aes(label = terms), 
            hjust = -0.05) +              # 단어를 막대도표 바깥으로 이동
  scale_x_continuous(expand = c(0, 0),    # x축 막대 위치를 Y축쪽으로 조정
                     limit = c(0, 1)) +   # x축 범위 설정
  labs(x = expression("문서 확률분포"~(gamma)), y = NULL,
       title = "인공지능 관련보도 상위 주제어",
       subtitle = "주제별로 기여도가 높은 단어 중심") +
  theme(plot.title = element_text(size = 20))
  
```



### 과제 

1. 관심있는 검색어를 이용해 빅카인즈에서 기사를 검색해 수집한 기사의 주제모형을 구축한다.

2. 구축한 주제 모형에 대해 시각화한다(주제별 단어분포, 주제별 문서분포, 주제별 단어-문서 분포)



